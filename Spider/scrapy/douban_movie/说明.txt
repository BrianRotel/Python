
应用Scrapy爬虫框架主要是编写--配置型代码

安装执行过程中的问题：
1. 安装pywin32
2. spiders文件夹中的spider.py文件用来爬取数据；pipelines.py文件用来处理数据
3. 引入外部python文件中的类，实例化对象
4. Selectors选择器分析网页内容，分析过程类似beautifulsoup、lxml等解析器
    4.1 selectors解析，结合xpath，extract()提取内容：
        sel = Selector(response)
        sel.xpath('***').extract()
    4.2 lxml解析，结合xpath提取内容：
        HTML = etree.HTML(response.body)    # body: a str containing the body of this Response
        HTML.xpath('***')
5. xpath解析时，使用extract()的多种情况：
    sel.xpath('***')                    # 返回一个SelectorList对象，selector list类是内建list类的子类
    sel.xpath('***').extract()          # 返回结果为单一化的unicode字符串列表list，即需要提取的内容
    sel.xpath('***').extract()[0]       # 返回list中的第一个元素（若list为空则抛异常）
    sel.xpath('***')[0].extract()       # 返回SelectorList中的第一个元素（若list为空则抛异常），与 sel.xpath('***').extract()[0] 效果一致
    sel.xpath('***')[0].extract()[0]    # 返回SelectorList中的第一个元素（str类型）的第一个字符
6. yield生成器
7. 运行爬虫工程时，打开第二级文件夹对应的终端
8. spider.py类中的 name = "douban" 与运行爬虫时的 scrapy crawl douban 命令中的douban一致


新建爬虫工程步骤：
1. 建立一个Scrapy爬虫工程。
    选取一个目录（**\scrapy\），执行指令：
    **\scrapy>scrapy startproject [projectname]
2. 在工程中产生一个Scrapy爬虫模板。
    进入工程目录（**\scrapy\[projectname]），执行指令：
    **\scrapy\[projectname]>scrapy genspider [spidername] [projectname].io
    作用：
    a. 生成一个名称为[spidername]的spider爬虫
    b. 在spiders目录下增加代码文件[spidername].py

    注：
    a. 在[spidername].py文件中的[spidername]Spider类中：
        name为执行爬虫对应的名称；
        allowed_domains为爬取网站的域名；
        start_urls为开始爬取的链接；
        parse()用于处理响应，解析内容形成字典；发现新的URL爬取请求；设置为start_requests()的回调函数
3. 编写产生的Spider爬虫模板。
    对于[spidername].py文件中的[spidername]Spider类：
        初始URL地址；
        编写获取页面后的解析方式（selector、bs4、lxml）；
4. 编写Item Pipeline。
    Item对象表示一个从HTML页面中提取的信息内容，由Spider生成，由pipeline.py文件中的pipeline类process_item()处理。
    Item类可以在框架自动生成的items.py文件中定义（根据需要爬取内容的结构指定），指定生成的item对象；或者 在parse()中将返回结果封装成字典形式，在pipeline类中处理提取到的信息。
    若Item类在items.py文件中自定义，则返回自定义的Item类型到pipeline中处理。
5. 优化配置策略。
    a. Configure item pipelines
6. 运行爬虫，获取信息。
    在第二级文件夹对应的命令行下，执行命令：
    **\scrapy\[projectname]>scrapy crawl [[spidername]Spider类中的name值]
    爬虫运行，显示或保存爬取结果。


主要修改三块内容：
1. 爬虫文件scrapy.py
    request对象发送请求，response对象返回响应，item对象处理提取信息
2. 处理文件pipelines.py
    接收item对象并处理，存入数据库或存入文件
3. 配置文件settings.py
    优化配置策略


待解决问题:
1. 爬取多个url链接--结合生成器
2. 数据存入数据库或写入特定的文件


Scrapy爬虫的数据类型：
注意三种类型内在的主要方法！
1. request类
    class scrapy.http.Request()
    request对象表示一个HTTP请求。由spider生成，由downloader执行
2. response类
    class scrapy.http.Response()
    response对象表示一个HTTP响应。由download生成，由spider处理
3. item类
    class scrapy.item.Item()
    item对象表示一个从HTML页面中提取的信息内容。由spider生成，由item pipeline处理
    item类似字典类型，可以按照字典类型操作


Spiders:
（spiders文件夹下的spider.py文件）
Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。
换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方（spiders文件夹下的spider.py文件）。
对Spider来说，爬取的细节过程如下：
1. 以初始的URL初始化Request，并设置回调函数。
    当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。
    spider中初始的request是通过调用 start_requests() 来获取的。
    start_requests() 读取 start_urls 中的URL， 并以 parse 为回调函数生成 Request 。
2. 在回调函数内分析返回的(网页)内容，返回 Item 对象或者 Request 或者一个包括二者的可迭代容器。
    返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。
3. 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，
    并根据分析的数据生成item。
4. 最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。


参考网址：
scrapy入门教程---http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html?highlight=response#


